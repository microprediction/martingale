import numpy as np
import multiprocessing as mp
from martingale.stats.fewvar import FEWVar
from martingale.strhash import str_hash


def evaluator(gen, model_cls_list, n=2000, epoch_len=200, epoch_fading_factor=0.1, burn_in=10000):
    """
    Evaluate multiple models on a process generated by `gen`.
    We:
    - Burn in the generator for `burn_in` steps.
    - Compute ergodic std of dx during burn-in for normalization.
    - Run the next `n` steps, updating models and recording normalized errors.
    - Emit per-epoch results, each with a decreasing epoch_importance.
    Returns a list of epoch result dicts.
    """
    error_fading_factor = 1 / epoch_len

    # Burn-in: Collect data to compute ergodic std
    burn_in_x = []
    count = 0
    epoch = 0

    stream = gen(n=burn_in+n)   # Instantiate generator with default params

    for obs in stream:
        burn_in_x.append(obs['x'])
        count += 1
        if count >= burn_in:
            break

    burn_in_x = np.array(burn_in_x)

    # Compute the ergodic standard deviation in changes ... we'll be using this to normalize
    dx = np.diff(burn_in_x)
    ergodic_std = np.std(dx)
    if ergodic_std == 0:
       return []   #  Dangerous to use this in any evaluation


    # Now run the next n steps, reporting rolling squared errors
    steps = 0
    epoch_importance = 1
    all_epoch_results = []
    stream_hash = str_hash()  # random hash string
    num_models = len(model_cls_list)
    models = [cls() for cls in model_cls_list]
    model_performance = [FEWVar(fading_factor=error_fading_factor) for _ in models]
    prev_means = [np.nan for _ in range(num_models)]

    for obs in stream:
        steps += 1
        x_true = obs['x']

        if np.any(np.isnan(prev_means)):
            if steps>1:
                print('A nan was present, so judging none')
            for m_idx, model in enumerate(models):
                model.update(x=x_true)
                prev_means[m_idx] = model.get_mean()
            continue

        if steps >= n+1:
            break

        for m_idx, model in enumerate(models):
            prev_mean = prev_means[m_idx]
            error = (x_true - prev_mean) / ergodic_std
            model_performance[m_idx].update(error * error)
            model.update(x=x_true)
            prev_means[m_idx] = model.get_mean()

        if steps % epoch_len == 0:
            epoch += 1
            epoch_results = []
            for m_idx, cls in enumerate(model_cls_list):
                mean_sqr_err = model_performance[m_idx].get_mean()
                epoch_results.append({
                    'model': cls.__name__,
                    'stream': stream_hash,
                    'epoch': epoch,
                    'mean_error': mean_sqr_err,
                    'epoch_importance': epoch_importance
                })

            epoch_importance *= epoch_fading_factor
            all_epoch_results.extend(epoch_results)
    return all_epoch_results


def worker_evaluator(gen_factory, model_cls_list, n, burn_in, result_queue):
    """
    Worker process function:
    - Creates a new generator from gen_factory
    - Calls evaluator
    - Puts results in result_queue
    """
    gen = gen_factory()
    res = evaluator(gen, model_cls_list, n=n, burn_in=burn_in)
    result_queue.put(res)


def benchmark(gen_factory, model_cls_list, runs=4, n=200, burn_in=9800):
    """
    Benchmark multiple models by running multiple independent realizations
    of the process in parallel using multiprocessing.

    gen_factory: a callable that returns a new generator each time it's called.
    model_cls_list: list of model classes
    runs: number of independent runs
    n, burn_in: parameters passed to evaluator
    """

    result_queue = mp.Queue()
    processes = []

    for _ in range(runs):
        p = mp.Process(target=worker_evaluator, args=(gen_factory, model_cls_list, n, burn_in, result_queue))
        p.start()
        processes.append(p)

    # Collect results
    all_results = []
    for _ in range(runs):
        res = result_queue.get()
        all_results.append(res)

    # Wait for all processes to finish
    for p in processes:
        p.join()

    # all_results is a list of runs, each run is a list of epoch_results
    # epoch_results is a dict: { model_name: { 'mean_error', 'var_error', 'epoch_importance', ...}, ... }

    # We want to aggregate across all runs and epochs using epoch_importance as weights
    # Initialize accumulators for each model
    acc_mean_error_weighted_sum = {model_cls.__name__: 0.0 for model_cls in model_cls_list}
    acc_weight_sum = {model_cls.__name__: 0.0 for model_cls in model_cls_list}

    # Aggregate over all runs and epochs
    for run_epochs in all_results:
        # run_epochs is a list of epoch dicts
        for epoch_result in run_epochs:
            # epoch_result is a dict keyed by model name
            for model_cls in model_cls_list:
                name = model_cls.__name__
                model_res = epoch_result[name]
                w = model_res['epoch_importance']
                acc_mean_error_weighted_sum[name] += model_res['mean_error'] * w
                acc_weight_sum[name] += w

    # Compute weighted averages
    aggregated = {}
    for model_cls in model_cls_list:
        name = model_cls.__name__
        total_weight = acc_weight_sum[name] if acc_weight_sum[name] != 0 else 1.0
        mean_error = acc_mean_error_weighted_sum[name] / total_weight
        aggregated[name] = {
            'mean_error': mean_error,
            'std_error': np.sqrt(mean_error)
        }

    return aggregated

# Example usage (adjust to your actual environment):
# Suppose we have a factory function that returns a new generator:
# def gen_factory():
#     params = brown_parameters()
#     return brown_gen(n=20000, params=params)

# Suppose we have a list of model classes:
# model_cls_list = [MyModelClass1, MyModelClass2, MyModelClass3]

# Now we can run:
# results = benchmark(gen_factory, model_cls_list, runs=4, n=200, burn_in=9800)
# print("Benchmark Results:", results)
