# evaluation.py
import numpy as np
from martingale.stats.fewvar import FEWVar
from martingale.strhash import str_hash

def evaluator(gen, model_cls_list, n=2000, epoch_len=200, epoch_fading_factor=0.1, burn_in=10000):
    """
    Evaluate multiple models on a process generated by `gen`.
    Returns a list of epoch result dicts:
    [
       {
         'model': model_name,
         'stream': stream_hash,
         'epoch': epoch_number,
         'mean_error': mean_squared_error,
         'epoch_importance': weight_for_this_epoch
       },
       ...
    ]
    """
    error_fading_factor = 1 / epoch_len

    # Burn-in: Collect data to compute ergodic std
    burn_in_x = []
    count = 0
    epoch = 0

    stream = gen(n=burn_in+n)   # Instantiate generator with default params

    for obs in stream:
        burn_in_x.append(obs['x'])
        count += 1
        if count >= burn_in:
            break

    burn_in_x = np.array(burn_in_x)
    dx = np.diff(burn_in_x)
    ergodic_std = np.std(dx)
    if ergodic_std == 0:
       return []   # Dangerous to use this in any evaluation

    steps = 0
    epoch_importance = 1
    all_epoch_results = []
    stream_hash = str_hash()  # random hash string
    num_models = len(model_cls_list)
    models = [cls() for cls in model_cls_list]
    model_performance = [FEWVar(fading_factor=error_fading_factor) for _ in models]
    prev_means = [np.nan for _ in range(num_models)]

    for obs in stream:
        steps += 1
        x_true = obs['x']

        if np.any(np.isnan(prev_means)):
            # initialize models
            for m_idx, model in enumerate(models):
                model.update(x=x_true)
                prev_means[m_idx] = model.get_mean()
            continue

        if steps >= n+1:
            break

        for m_idx, model in enumerate(models):
            prev_mean = prev_means[m_idx]
            error = (x_true - prev_mean) / ergodic_std
            model_performance[m_idx].update(error * error)
            model.update(x=x_true)
            prev_means[m_idx] = model.get_mean()

        if steps % epoch_len == 0:
            epoch += 1
            epoch_results = []
            for m_idx, cls in enumerate(model_cls_list):
                mean_sqr_err = model_performance[m_idx].get_mean()
                epoch_results.append({
                    'model': cls.__name__,
                    'stream': stream_hash,
                    'epoch': epoch,
                    'mean_error': mean_sqr_err,
                    'epoch_importance': epoch_importance
                })

            epoch_importance *= epoch_fading_factor
            all_epoch_results.extend(epoch_results)

    return all_epoch_results

if __name__ == "__main__":
    # Simple single-thread test
    def simple_gen(n=10000):
        x = 0
        for i in range(n):
            x += np.random.randn()
            yield {'x': x}

    class SimpleModel:
        def __init__(self):
            self.mean = 0.0
            self.count = 0
        def update(self, x):
            self.count += 1
            self.mean += (x - self.mean) / self.count
        def get_mean(self):
            return self.mean

    results = evaluator(simple_gen, [SimpleModel], n=200, epoch_len=50, epoch_fading_factor=0.5, burn_in=1000)
    print("Single-threaded test results:", results)
